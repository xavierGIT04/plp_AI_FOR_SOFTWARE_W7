{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# --- Part 3: Practical Audit (COMPAS Recidivism Dataset) ---\n",
    "# This script simulates the analysis of racial bias using the principles and metrics\n",
    "# found in the IBM AI Fairness 360 (AIF360) toolkit.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# In a real environment, you would import:\n",
    "# from aif360.datasets import StandardDataset\n",
    "# from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "# from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# --- 1. Simulate Loading and Pre-processing Data ---\n",
    "# AIF360 requires specific column names for protected attributes and favorable labels.\n",
    "\n",
    "# Create simulated COMPAS data based on ProPublica's findings\n",
    "data_size = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Protected Attribute: Race (0=Black, 1=White)\n",
    "race = np.random.choice([0, 1], size=data_size, p=[0.5, 0.5])\n",
    "\n",
    "# Favorable Label: Non-Recidivism (0=Recidivate, 1=No Recidivate)\n",
    "# Note: In the COMPAS context, a 'High Risk' score (Recidivate=0) is the unfavorable outcome.\n",
    "true_labels = np.random.choice([0, 1], size=data_size, p=[0.45, 0.55])\n",
    "\n",
    "# Simulated Prediction Score (0=High Risk/Recidivate, 1=Low Risk/No Recidivate)\n",
    "# SIMULATED BIAS: Assume model is more likely to assign high risk (0) to Black individuals (0)\n",
    "predictions = np.zeros(data_size)\n",
    "\n",
    "# If race is Black (0): Assign Recidivism (0) 70% of the time, Low Risk (1) 30%\n",
    "predictions[race == 0] = np.random.choice([0, 1], size=np.sum(race == 0), p=[0.70, 0.30])\n",
    "\n",
    "# If race is White (1): Assign Recidivism (0) 40% of the time, Low Risk (1) 60%\n",
    "predictions[race == 1] = np.random.choice([0, 1], size=np.sum(race == 1), p=[0.40, 0.60])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'race': race,\n",
    "    'true_label': true_labels,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# --- 2. Define Protected and Unprivileged Groups (AIF360 Structure) ---\n",
    "# The 'favorable' outcome is Non-Recidivism (Low Risk), coded as 1.\n",
    "# The 'unprivileged' group is Black (0).\n",
    "privileged_groups = [{'race': 1}]  # White\n",
    "unprivileged_groups = [{'race': 0}] # Black\n",
    "favorable_label = 1\n",
    "\n",
    "# --- 3. Calculate Bias Metrics (Simulated using Pandas) ---\n",
    "\n",
    "# --- Disparate Impact Ratio (DIR) ---\n",
    "# DIR = (Selection Rate for Unprivileged Group) / (Selection Rate for Privileged Group)\n",
    "# Selection Rate = P(prediction=1)\n",
    "rate_black = df[df['race'] == 0]['prediction'].mean()\n",
    "rate_white = df[df['race'] == 1]['prediction'].mean()\n",
    "\n",
    "dir_value = rate_black / rate_white if rate_white != 0 else 0\n",
    "\n",
    "# --- Equal Opportunity Difference (EOD) ---\n",
    "# EOD = (True Positive Rate for Unprivileged Group) - (True Positive Rate for Privileged Group)\n",
    "# TPR (True Positive Rate) = P(prediction=1 | true_label=1) -- Correctly predicting Low Risk\n",
    "\n",
    "# Calculate TPR for Black group (where true label is 1 / Low Risk)\n",
    "black_low_risk_actual = df[(df['race'] == 0) & (df['true_label'] == favorable_label)]\n",
    "tpr_black = black_low_risk_actual['prediction'].sum() / len(black_low_risk_actual) if len(black_low_risk_actual) > 0 else 0\n",
    "\n",
    "# Calculate TPR for White group (where true label is 1 / Low Risk)\n",
    "white_low_risk_actual = df[(df['race'] == 1) & (df['true_label'] == favorable_label)]\n",
    "tpr_white = white_low_risk_actual['prediction'].sum() / len(white_low_risk_actual) if len(white_low_risk_actual) > 0 else 0\n",
    "\n",
    "eod_value = tpr_black - tpr_white\n",
    "\n",
    "# --- 4. Print Results ---\n",
    "print(\"--- COMPAS Bias Audit Results (Racial Bias) ---\")\n",
    "print(f\"Privileged Group (White Prediction Rate): {rate_white:.2f}\")\n",
    "print(f\"Unprivileged Group (Black Prediction Rate): {rate_black:.2f}\")\n",
    "print(f\"Disparate Impact Ratio (DIR): {dir_value:.2f}\")\n",
    "\n",
    "# Ideal DIR is 1.0 (Fairness range: 0.8 to 1.25).\n",
    "if dir_value < 0.8 or dir_value > 1.25:\n",
    "    print(\"STATUS: Bias Detected (Fails Statistical Parity)\")\n",
    "else:\n",
    "    print(\"STATUS: Fair (Passes Statistical Parity)\")\n",
    "\n",
    "print(\"\\n--- Equal Opportunity (Recidivism) ---\")\n",
    "print(f\"True Positive Rate (TPR) for Black Group: {tpr_black:.2f}\")\n",
    "print(f\"True Positive Rate (TPR) for White Group: {tpr_white:.2f}\")\n",
    "print(f\"Equal Opportunity Difference (EOD): {eod_value:.2f}\")\n",
    "\n",
    "# Ideal EOD is 0.0.\n",
    "if abs(eod_value) > 0.1: # Threshold for fairness\n",
    "    print(\"STATUS: Significant Bias Detected (Fails Equal Opportunity)\")\n",
    "else:\n",
    "    print(\"STATUS: Acceptable (Passes Equal Opportunity)\")\n",
    "\n",
    "# --- 5. Remediation Concept (If this were a real AIF360 application) ---\n",
    "# remediation = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "#                          privileged_groups=privileged_groups)\n",
    "# remediated_data = remediation.fit_transform(data)\n",
    "# model.fit(remediated_data)\n",
    "print(\"\\nRemediation Step: Reweighing (Pre-processing mitigation) would be applied to assign\")\n",
    "print(\"higher weights to the underrepresented or unfairly treated subgroups in the training data.\")"
   ],
   "metadata": {
    "id": "PjSzgB6Vng6E",
    "ExecuteTime": {
     "end_time": "2025-10-15T20:47:28.790535Z",
     "start_time": "2025-10-15T20:47:21.047069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COMPAS Bias Audit Results (Racial Bias) ---\n",
      "Privileged Group (White Prediction Rate): 0.59\n",
      "Unprivileged Group (Black Prediction Rate): 0.29\n",
      "Disparate Impact Ratio (DIR): 0.49\n",
      "STATUS: Bias Detected (Fails Statistical Parity)\n",
      "\n",
      "--- Equal Opportunity (Recidivism) ---\n",
      "True Positive Rate (TPR) for Black Group: 0.32\n",
      "True Positive Rate (TPR) for White Group: 0.56\n",
      "Equal Opportunity Difference (EOD): -0.24\n",
      "STATUS: Significant Bias Detected (Fails Equal Opportunity)\n",
      "\n",
      "Remediation Step: Reweighing (Pre-processing mitigation) would be applied to assign\n",
      "higher weights to the underrepresented or unfairly treated subgroups in the training data.\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
